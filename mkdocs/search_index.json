{
    "docs": [
        {
            "location": "/", 
            "text": "Note\n\n\n\n\nThis test drive is currently in its BETA version. Please send any bugs or documentation errors to \nkaran@redhat.com\n\n\n\n\n\n\nIntroduction\n#\n\n\nWelcome to Red Hat Ceph Storage Hands-on Lab. To make your Ceph experience awesome , the contents of this test drive have been divided into following modules.\n\n\n\n\nModule 1 :\n Setting up a Ceph cluster\n\n\nModule 2 :\n Accessing Ceph cluster via Block Storage interface\n\n\nModule 3 :\n Accessing Ceph cluster via Object Storage interface\n\n\nModule 4 :\n Scaling up a Ceph cluster\n\n\n\n\nTest Drive Prerequisites\n#\n\n\n\n\nFor Windows users :\n you will need a Secure Shell client like PuTTY to connect to your instance. If you do not have it already, you can download the PuTTY client \nhere\n\n\nFor Mac and Linux users:\n  you will need a Terminal application to SSH into LAB machine (this should already be installed on your host). \n\n\n\n\nKnow your Lab Environment\n#\n\n\nStarting the LAB\n#\n\n\n\n\nTo launch your LAB Environment, click \nStart Lab\n button from the Lab panel. Hang tight , it will take \n12-15 minutes\n to launch LAB resources.\n\n\nOnce LAB is launched, grab the \nManagement Node IP\n from \nAddl. Info\n tab on Right Side\n\n\n\n\n\n\nNote\n\n\n\n\nIf the \nAddl. Info\n Tab is unavailable, make sure you clicked \nStart Lab\n button on the top bar of interface.\n\n\nAfter clicking \nStart Lab\n , Lab creation will take some \n12-15 minutes\n. This is expected,  so Hang Tight.\n\n\n\n\n\n\n\n\nOn the \nLab Details\n tab, notice the lab properties:\n\n\nSetup Time -\n The estimated time for the lab to start your instance so you can access the lab environment.\n\n\nDuration -\n The estimated time the lab should take to complete.\n\n\nAccess -\n The time lab will run before automatically shutting down.\n\n\n\n\n\n\n\n\nAccessing the LAB\n#\n\n\n\n\nOpen SSH client software on your workstation.\n\n\nAs \nceph\n user, SSH into Ceph Management node by using the \nManagement node IP address\n which you can get from the \nAddl. Info\n tab.\n\n\nSSH into the node using the following command and provided credentials\n\n\n$ ssh ceph@\nMangement Node IP Address\n\n\nLogin Credentials\n \u21d2 \nUser Name:\n \nceph\n and \nPassword:\n \nRedhat16\n\n\n\n\nExample Snippet:\n\n\n$ ssh ceph@52.55.48.166\nThe authenticity of host '52.55.48.166 (52.55.48.166)' can't be established.\nECDSA key fingerprint is 23:36:98:4f:18:cc:60:98:35:34:58:8e:85:46:67:66.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '52.55.48.166' (ECDSA) to the list of known hosts.\nceph@52.55.48.166's password:\nLast login: Fri Oct  7 05:47:16 2016 from a91-156-42-216.elisa-laajakaista.fi\n[ceph@mgmt ~]$\n\n\n\n\n\n\nNote\n\n\n\n\n If your SSH connection fails , do the following : \n \n\n\nSelect the CONNECTION tab from the main lab page \n\n\nSelect the Download PEM/PPK button\n\n\nSelect Download PEM , note the download location of the PEM file and execute the following\n\n\nssh -i absolute-path-of-pem-file ceph@Management-node-ip-address \n\n\n\n\n\n\nTerminating the LAB\n#\n\n\nFollow these steps to end your lab environment and share your experience.\n\n\n\n\nClose your remote session.\n\n\nIn the \nqwik\nLABS page, click \nEnd Lab\n.\n\n\nIn the confirmation message, click \nOK\n.\n\n\nTell us about your lab experience on Ceph and suggestions to improve this environment\n\n\n\n\nRed Hat Ceph Storage\n#\n\n\nRed Hat Ceph Storage ( RHCS ) is a scalable, open, software-defined storage platform that combines the most stable version of the Ceph storage system with a Ceph management platform, deployment utilities, and support services. \n\n\nRed Hat Ceph Storage is designed for cloud infrastructure and web-scale object storage. Red Hat Ceph Storage clusters consist of the following types of nodes:\n\n\n\n\nRed Hat Storage Management node\n\n\nMonitor nodes\n\n\nOSD nodes\n\n\nObject Gateway nodes\n\n\nMDS nodes\n\n\n\n\nRHCS Prerequisites\n#\n\n\nSetting up a RHCS requires the following configuration on the cluster nodes.\n\n\n\n\nOperating System :\n  Heterogeneous and supported version of OS.\n\n\nRegistration to RHN :\n To get necessary packages required for installation either from RHN (Red Hat Network) or other trusted sources.\n\n\nEnabling Ceph Repositories :\n To get Ceph packages from CDN or from Local repositories.\n\n\nSeparate networks :\n For Public and Cluster traffic.\n\n\nSetting hostname resolutions :\n Either local or DNS name resolution \n\n\nConfiguring firewall :\n  Allow necessary port to be opened.\n\n\nNTP configuration:\n For time synchronization across nodes.\n\n\nLocal user account:\n User with passwordless sudo ssh access to all nodes, for Ceph deployment using Ansible.\n\n\n\n\n\n\nNote\n\n\nThe purpose of Red Hat Ceph Storage Test Drive is to provide you an environment where you can focus on learning this great technology, without spending any time in fulfilling prerequisites. All the prerequisites listed above have been taken care for rest of this course.\n\n\n\n\nSo without further delay, please go ahead and follow these modules to gain experience on Ceph.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to Red Hat Ceph Storage Hands-on Lab. To make your Ceph experience awesome , the contents of this test drive have been divided into following modules.   Module 1 :  Setting up a Ceph cluster  Module 2 :  Accessing Ceph cluster via Block Storage interface  Module 3 :  Accessing Ceph cluster via Object Storage interface  Module 4 :  Scaling up a Ceph cluster", 
            "title": "Introduction"
        }, 
        {
            "location": "/#test-drive-prerequisites", 
            "text": "For Windows users :  you will need a Secure Shell client like PuTTY to connect to your instance. If you do not have it already, you can download the PuTTY client  here  For Mac and Linux users:   you will need a Terminal application to SSH into LAB machine (this should already be installed on your host).", 
            "title": "Test Drive Prerequisites"
        }, 
        {
            "location": "/#know-your-lab-environment", 
            "text": "", 
            "title": "Know your Lab Environment"
        }, 
        {
            "location": "/#starting-the-lab", 
            "text": "To launch your LAB Environment, click  Start Lab  button from the Lab panel. Hang tight , it will take  12-15 minutes  to launch LAB resources.  Once LAB is launched, grab the  Management Node IP  from  Addl. Info  tab on Right Side    Note   If the  Addl. Info  Tab is unavailable, make sure you clicked  Start Lab  button on the top bar of interface.  After clicking  Start Lab  , Lab creation will take some  12-15 minutes . This is expected,  so Hang Tight.     On the  Lab Details  tab, notice the lab properties:  Setup Time -  The estimated time for the lab to start your instance so you can access the lab environment.  Duration -  The estimated time the lab should take to complete.  Access -  The time lab will run before automatically shutting down.", 
            "title": "Starting the LAB"
        }, 
        {
            "location": "/#accessing-the-lab", 
            "text": "Open SSH client software on your workstation.  As  ceph  user, SSH into Ceph Management node by using the  Management node IP address  which you can get from the  Addl. Info  tab.  SSH into the node using the following command and provided credentials  $ ssh ceph@ Mangement Node IP Address  Login Credentials  \u21d2  User Name:   ceph  and  Password:   Redhat16   Example Snippet:  $ ssh ceph@52.55.48.166\nThe authenticity of host '52.55.48.166 (52.55.48.166)' can't be established.\nECDSA key fingerprint is 23:36:98:4f:18:cc:60:98:35:34:58:8e:85:46:67:66.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '52.55.48.166' (ECDSA) to the list of known hosts.\nceph@52.55.48.166's password:\nLast login: Fri Oct  7 05:47:16 2016 from a91-156-42-216.elisa-laajakaista.fi\n[ceph@mgmt ~]$   Note    If your SSH connection fails , do the following :     Select the CONNECTION tab from the main lab page   Select the Download PEM/PPK button  Select Download PEM , note the download location of the PEM file and execute the following  ssh -i absolute-path-of-pem-file ceph@Management-node-ip-address", 
            "title": "Accessing the LAB"
        }, 
        {
            "location": "/#terminating-the-lab", 
            "text": "Follow these steps to end your lab environment and share your experience.   Close your remote session.  In the  qwik LABS page, click  End Lab .  In the confirmation message, click  OK .  Tell us about your lab experience on Ceph and suggestions to improve this environment", 
            "title": "Terminating the LAB"
        }, 
        {
            "location": "/#red-hat-ceph-storage", 
            "text": "Red Hat Ceph Storage ( RHCS ) is a scalable, open, software-defined storage platform that combines the most stable version of the Ceph storage system with a Ceph management platform, deployment utilities, and support services.   Red Hat Ceph Storage is designed for cloud infrastructure and web-scale object storage. Red Hat Ceph Storage clusters consist of the following types of nodes:   Red Hat Storage Management node  Monitor nodes  OSD nodes  Object Gateway nodes  MDS nodes", 
            "title": "Red Hat Ceph Storage"
        }, 
        {
            "location": "/#rhcs-prerequisites", 
            "text": "Setting up a RHCS requires the following configuration on the cluster nodes.   Operating System :   Heterogeneous and supported version of OS.  Registration to RHN :  To get necessary packages required for installation either from RHN (Red Hat Network) or other trusted sources.  Enabling Ceph Repositories :  To get Ceph packages from CDN or from Local repositories.  Separate networks :  For Public and Cluster traffic.  Setting hostname resolutions :  Either local or DNS name resolution   Configuring firewall :   Allow necessary port to be opened.  NTP configuration:  For time synchronization across nodes.  Local user account:  User with passwordless sudo ssh access to all nodes, for Ceph deployment using Ansible.    Note  The purpose of Red Hat Ceph Storage Test Drive is to provide you an environment where you can focus on learning this great technology, without spending any time in fulfilling prerequisites. All the prerequisites listed above have been taken care for rest of this course.   So without further delay, please go ahead and follow these modules to gain experience on Ceph.", 
            "title": "RHCS Prerequisites"
        }, 
        {
            "location": "/Module-1/", 
            "text": "Module - 1 : Setting up a Ceph cluster\n#\n\n\nRHCS 2.0 has introduced a new and more efficient way to deploy Ceph cluster. Instead of \nceph-deploy\n  RHCS 2.0 ships with \nceph-ansible\n tool which is based on configuration management tool \nAnsible\n .\n\n\nIn this module we will deploy a Ceph cluster with 3 OSD nodes and 3 Monitor nodes. We will use \nceph-ansible\n to deploy this cluster.\n\n\n\n\nNote\n\n\nYou must run all the commands using \nceph\n user and from \nmanagement node\n, unless otherwise specified. \n\n\n\n\n\n\nFrom your workstation login to the Ceph management node as \nceph\n user\n\n\n\n\n$ ssh ceph@\nIP address of Ceph Management node\n\n\n\n\nInstalling and setting up ceph-ansible\n#\n\n\n\n\nInstall the ceph-ansible package\n\n\n\n\n$ sudo yum install -y ceph-ansible\n\n\n\n\n\n\nTo keep ansible hosts file short, rename the default ansible host file\n\n\n\n\n$ sudo mv /etc/ansible/hosts /etc/ansible/hosts-default.bkp\n\n\n\n\n\n\nCreate a new ansible hosts file, with the following edits \n\n\n\n\n$ sudo vi /etc/ansible/hosts\n\n\n\n\n\n\nIn the \n/etc/ansible/hosts\n file add Ceph monitor host names under \n[mons]\n  section and Ceph OSDs host name under \n[osds]\n  section . This allows ansible to know which role is to be applied\n\n\n\n\n[mons]\nmon-node1\nmon-node2\nmon-node3\n[osds]\nosd-node1\nosd-node2\nosd-node3\n\n\n\n\n\n\nCreate the \n.ansible.cfg\n file and add \nhost_key_checking = False\n using the commands below \n\n\n\n\n$ echo \n[defaults]\n \n /home/ceph/.ansible.cfg\n$ echo \nhost_key_checking = False\n \n /home/ceph/.ansible.cfg\n\n\n\n\n\n\nEnsure that Ansible can reach the Ceph hosts.\n\n\n\n\n$ ansible all -m ping\n\n\n\n\nConfiguring Ceph Global Settings\n#\n\n\n\n\nCreate a directory under the home directory so Ansible can write the keys\n\n\n\n\n$ cd ~\n$ mkdir ceph-ansible-keys\n\n\n\n\n\n\nNavigate to the Ceph Ansible \n`group_vars\n directory\n\n\n\n\n$ cd /usr/share/ceph-ansible/group_vars/\n\n\n\n\n\n\nCreate an \nall\n file from the \nall.sample\n  file and open it for editing\n\n\n\n\n$ sudo cp all.sample all\n$ sudo vi all\n\n\n\n\n\n\nUncomment \nfetch_directory\n setting under the \nGENERAL\n section and point it to directory we created previously for ceph-ansible-keys\n\n\n\n\nfetch_directory: ~/ceph-ansible-keys\n\n\n\n\n\n\nUnder \nStable Releases\n section and \nENTERPRISE VERSION RED HAT STORAGE\n subsection, uncomment \nceph_stable_rh_storage\n setting and set it to \ntrue\n\n\n\n\nceph_stable_rh_storage: true\n\n\n\n\n\n\nUncomment the \nceph_stable_rh_storage_iso_install\n setting and set it to \ntrue\n \n\n\n\n\nceph_stable_rh_storage_iso_install: true\n\n\n\n\n\n\nUncomment the \nceph_stable_rh_storage_iso_path\n setting and specify the path to RHCS 2.0 ISO image\n\n\n\n\nceph_stable_rh_storage_iso_path: /home/ceph/rhceph-2.0-rhel-7-x86_64.iso\n\n\n\n\n\n\nUncomment the \ncephx\n setting under \nCEPH CONFIGURATION\n section\n\n\n\n\ncephx: true\n\n\n\n\n\n\nUncomment the \nmonitor_interface\n setting under \nMonitor options\n section and specify monitor node interface name.\n\n\n\n\nmonitor_interface: eth0\n\n\n\n\n\n\nSet the \njournal_size\n  setting\n\n\n\n\njournal_size: 4096\n\n\n\n\n\n\nSet the \npublic_network\n and \ncluster_network\n settings\n\n\n\n\npublic_network: 10.100.2.0/24\ncluster_network: 10.100.1.0/24\n\n\n\n\n\n\nSave the file and exit from the editor\n\n\n\n\nConfiguring Ceph OSD Settings\n#\n\n\n\n\nTo disk devices as Ceph OSD, verify disks logical names. In most cases disk name should be \nxvdb\n \nxvdc\n and \nxvdd\n \n\n\n\n\n$ ssh osd-node1 lsblk\n\n\n\n\n\n\nTo define Ceph OSDs , navigate to the \n/usr/share/ceph-ansible/group_vars/\n  directory\n\n\n\n\n$ cd /usr/share/ceph-ansible/group_vars/\n\n\n\n\n\n\nCreate an \nosds\n file from \nosds.sample\n file and open it for editing\n\n\n\n\n$ sudo cp osds.sample osds\n$ sudo vi osds\n\n\n\n\n\n\nUncomment the \ncrush_location\n setting and the \nosd_crush_location\n setting\n\n\n\n\ncrush_location: false\nosd_crush_location: \n'root={{ ceph_crush_root }} rack={{ ceph_crush_rack }} host={{ ansible_hostname }}'\n\n\n\n\n\n\nTo add OSD devices, uncomment the \ndevices:\n section and add the OSD devices logical name \n/dev/xvdb\n and \n/dev/xvdc\n and \n/dev/xvdd\n to the list of devices\n\n\n\n\ndevices:\n  - /dev/xvdb\n  - /dev/xvdc\n  - /dev/xvdd\n\n\n\n\n\n\nUncomment the \njournal_collocation\n setting and specify \ntrue\n so that OSDs can use co-located journals\n\n\n\n\njournal_collocation: true\n\n\n\n\nDeploying the Ceph Cluster\n#\n\n\n\n\nNavigate to the \nceph-ansible\n configuration directory\n\n\n\n\n$ cd /usr/share/ceph-ansible\n\n\n\n\n\n\nCreate a \nsite.yml\n file from the \nsite.yml.sample\n file \n\n\n\n\n$ sudo cp site.yml.sample site.yml\n\n\n\n\n\n\nRun the Ansible playbook\n\n\n\n\n$ ansible-playbook site.yml -u ceph\n\n\n\n\n\n\nAnsible will take a few minutes to complete Ceph deployment. Once its completed, Ansible play recap should look similar to this. Make sure Ansible Play Recap does not show any host run failed.\n\n\n\n\nPLAY RECAP ********************************************************************\nmon-node1                  : ok=91   changed=19   unreachable=0    failed=0\nmon-node2                  : ok=91   changed=18   unreachable=0    failed=0\nmon-node3                  : ok=91   changed=18   unreachable=0    failed=0\nosd-node1                  : ok=164  changed=16   unreachable=0    failed=0\nosd-node2                  : ok=164  changed=16   unreachable=0    failed=0\nosd-node3                  : ok=164  changed=16   unreachable=0    failed=0\n\n\n\n\n\n\nFinally check status of your cluster. You should have an installed and configured Ceph cluster at this point. \n\n\n\n\n$ ssh mon-node1 ceph -s\n\n\n\n\n\n\nNote\n\n\nYou can ignore any cluster health warnings at this point, We will take care of them later in this module.\n\n\n\n\n\n\nUpto this point you should have a running Ceph cluster with 3 Ceph OSD nodes ( 9 OSDs total ) and 3 Ceph Monitor nodes.\n \n\n\n\n\nConfiguring a Ceph client\n#\n\n\nBy default Ceph monitor nodes are authorized to run Ceph administrative commands. For the sake of understanding how Ceph client is configured, In this section we will configure a \nmgmt\n node as our Ceph client node.\n\n\n\n\nOn the \nmgmt\nnode install \nceph-common\n package which provides the \nCeph CLI\n and other tools\n\n\n\n\n$ sudo yum install -y ceph-common\n\n\n\n\n\n\nChange ownership of the \n/etc/ceph\n directory\n\n\n\n\n$ sudo chown -R ceph:ceph /etc/ceph\n\n\n\n\n\n\nFrom \nmon-node1\n copy the Ceph configuration file (\nceph.conf\n) and the Ceph administration keyring (\nceph.client.admin.keyring\n) to the \nmgmt\n node\n\n\n\n\n$ ssh mon-node1 -t cat /etc/ceph/ceph.conf | tee /etc/ceph/ceph.conf\n\n\n\n\n$ ssh mon-node1 -t cat /etc/ceph/ceph.client.admin.keyring | tee /etc/ceph/ceph.client.admin.keyring\n\n\n\n\n$ chmod 400 /etc/ceph/ceph.client.admin.keyring\n$ sudo chown -R ceph:ceph /etc/ceph\n\n\n\n\n\n\nVerify \nmgmt\n node which is our Ceph client , can run Ceph commands\n\n\n\n\n[ceph@mgmt ~]$ ceph -s\n    cluster 32ab020c-e510-4884-ab0a-63944c2c6b35\n     health HEALTH_WARN\n            too few PGs per OSD (21 \n min 30)\n     monmap e1: 3 mons at {mon-node1=10.100.2.11:6789/0,mon-node2=10.100.2.12:6789/0,mon-node3=10.100.2.13:6789/0}\n            election epoch 6, quorum 0,1,2 mon-node1,mon-node2,mon-node3\n     osdmap e20: 9 osds: 9 up, 9 in\n            flags sortbitwise\n      pgmap v33: 64 pgs, 1 pools, 0 bytes data, 0 objects\n            300 MB used, 863 GB / 863 GB avail\n                  64 active+clean\n[ceph@mgmt ~]$\n\n\n\n\nInteracting with the Ceph cluster\n#\n\n\nIn this section we will learn a few commands to interact with our Ceph cluster. These commands should be executed from \nmon-node1\n node.\n\n\n\n\nssh to \nmon-node1\n \n\n\n\n\n$ ssh mon-node1\n\n\n\n\n\n\nCheck cluster status\n\n\n\n\n$ ceph -s\n\n\n\n\nThe Above cluster status command shows that the cluster health is not OK and the cluster is complaining about low PG numbers. \nLet's now try to fix this warning.\n\n\n\n\nVerify \npg_num\n for default pool \nrbd\n \n\n\n\n\n$ ceph osd dump | grep -i pool\n\n\n\n\n\n\nIncrease \npg_num\n  for \nrbd\n pool to 128 and check cluster status \n\n\n\n\n$ ceph osd pool set rbd pg_num 128\n$ ceph -s\n\n\n\n\n\n\nOnce the cluster is not creating new PGs , increase \npgp_num\n for \nrbd\n pool to 128 and check cluster status. Your cluster health should now report \nHEALTH_OK\n\n\n\n\n$ ceph osd pool set rbd pgp_num 128\n$ ceph -s\n\n\n\n\n\n\nCheck Ceph OSD stats and tree view of OSDs in cluster\n\n\n\n\n$ ceph osd stat\n$ ceph osd tree\n\n\n\n\n\n\nCheck Ceph monitor status\n\n\n\n\n$ ceph mon stat\n\n\n\n\n\n\nList and check Ceph pool status\n\n\n\n\n$ ceph osd lspools\n$ ceph df\n$ ceph osd dump | grep -i pool\n\n\n\n\n\n\nWe have reached the end of Module-1. At this point you have learned to deploy, configure and interact with your Ceph cluster.\nFollow the next module to learn how to access your Ceph cluster as a Block Storage.", 
            "title": "Module-1 Ceph Cluster Setup"
        }, 
        {
            "location": "/Module-1/#module-1-setting-up-a-ceph-cluster", 
            "text": "RHCS 2.0 has introduced a new and more efficient way to deploy Ceph cluster. Instead of  ceph-deploy   RHCS 2.0 ships with  ceph-ansible  tool which is based on configuration management tool  Ansible  .  In this module we will deploy a Ceph cluster with 3 OSD nodes and 3 Monitor nodes. We will use  ceph-ansible  to deploy this cluster.   Note  You must run all the commands using  ceph  user and from  management node , unless otherwise specified.     From your workstation login to the Ceph management node as  ceph  user   $ ssh ceph@ IP address of Ceph Management node", 
            "title": "Module - 1 : Setting up a Ceph cluster"
        }, 
        {
            "location": "/Module-1/#installing-and-setting-up-ceph-ansible", 
            "text": "Install the ceph-ansible package   $ sudo yum install -y ceph-ansible   To keep ansible hosts file short, rename the default ansible host file   $ sudo mv /etc/ansible/hosts /etc/ansible/hosts-default.bkp   Create a new ansible hosts file, with the following edits    $ sudo vi /etc/ansible/hosts   In the  /etc/ansible/hosts  file add Ceph monitor host names under  [mons]   section and Ceph OSDs host name under  [osds]   section . This allows ansible to know which role is to be applied   [mons]\nmon-node1\nmon-node2\nmon-node3\n[osds]\nosd-node1\nosd-node2\nosd-node3   Create the  .ansible.cfg  file and add  host_key_checking = False  using the commands below    $ echo  [defaults]    /home/ceph/.ansible.cfg\n$ echo  host_key_checking = False    /home/ceph/.ansible.cfg   Ensure that Ansible can reach the Ceph hosts.   $ ansible all -m ping", 
            "title": "Installing and setting up ceph-ansible"
        }, 
        {
            "location": "/Module-1/#configuring-ceph-global-settings", 
            "text": "Create a directory under the home directory so Ansible can write the keys   $ cd ~\n$ mkdir ceph-ansible-keys   Navigate to the Ceph Ansible  `group_vars  directory   $ cd /usr/share/ceph-ansible/group_vars/   Create an  all  file from the  all.sample   file and open it for editing   $ sudo cp all.sample all\n$ sudo vi all   Uncomment  fetch_directory  setting under the  GENERAL  section and point it to directory we created previously for ceph-ansible-keys   fetch_directory: ~/ceph-ansible-keys   Under  Stable Releases  section and  ENTERPRISE VERSION RED HAT STORAGE  subsection, uncomment  ceph_stable_rh_storage  setting and set it to  true   ceph_stable_rh_storage: true   Uncomment the  ceph_stable_rh_storage_iso_install  setting and set it to  true     ceph_stable_rh_storage_iso_install: true   Uncomment the  ceph_stable_rh_storage_iso_path  setting and specify the path to RHCS 2.0 ISO image   ceph_stable_rh_storage_iso_path: /home/ceph/rhceph-2.0-rhel-7-x86_64.iso   Uncomment the  cephx  setting under  CEPH CONFIGURATION  section   cephx: true   Uncomment the  monitor_interface  setting under  Monitor options  section and specify monitor node interface name.   monitor_interface: eth0   Set the  journal_size   setting   journal_size: 4096   Set the  public_network  and  cluster_network  settings   public_network: 10.100.2.0/24\ncluster_network: 10.100.1.0/24   Save the file and exit from the editor", 
            "title": "Configuring Ceph Global Settings"
        }, 
        {
            "location": "/Module-1/#configuring-ceph-osd-settings", 
            "text": "To disk devices as Ceph OSD, verify disks logical names. In most cases disk name should be  xvdb   xvdc  and  xvdd     $ ssh osd-node1 lsblk   To define Ceph OSDs , navigate to the  /usr/share/ceph-ansible/group_vars/   directory   $ cd /usr/share/ceph-ansible/group_vars/   Create an  osds  file from  osds.sample  file and open it for editing   $ sudo cp osds.sample osds\n$ sudo vi osds   Uncomment the  crush_location  setting and the  osd_crush_location  setting   crush_location: false\nosd_crush_location:  'root={{ ceph_crush_root }} rack={{ ceph_crush_rack }} host={{ ansible_hostname }}'   To add OSD devices, uncomment the  devices:  section and add the OSD devices logical name  /dev/xvdb  and  /dev/xvdc  and  /dev/xvdd  to the list of devices   devices:\n  - /dev/xvdb\n  - /dev/xvdc\n  - /dev/xvdd   Uncomment the  journal_collocation  setting and specify  true  so that OSDs can use co-located journals   journal_collocation: true", 
            "title": "Configuring Ceph OSD Settings"
        }, 
        {
            "location": "/Module-1/#deploying-the-ceph-cluster", 
            "text": "Navigate to the  ceph-ansible  configuration directory   $ cd /usr/share/ceph-ansible   Create a  site.yml  file from the  site.yml.sample  file    $ sudo cp site.yml.sample site.yml   Run the Ansible playbook   $ ansible-playbook site.yml -u ceph   Ansible will take a few minutes to complete Ceph deployment. Once its completed, Ansible play recap should look similar to this. Make sure Ansible Play Recap does not show any host run failed.   PLAY RECAP ********************************************************************\nmon-node1                  : ok=91   changed=19   unreachable=0    failed=0\nmon-node2                  : ok=91   changed=18   unreachable=0    failed=0\nmon-node3                  : ok=91   changed=18   unreachable=0    failed=0\nosd-node1                  : ok=164  changed=16   unreachable=0    failed=0\nosd-node2                  : ok=164  changed=16   unreachable=0    failed=0\nosd-node3                  : ok=164  changed=16   unreachable=0    failed=0   Finally check status of your cluster. You should have an installed and configured Ceph cluster at this point.    $ ssh mon-node1 ceph -s   Note  You can ignore any cluster health warnings at this point, We will take care of them later in this module.    Upto this point you should have a running Ceph cluster with 3 Ceph OSD nodes ( 9 OSDs total ) and 3 Ceph Monitor nodes.", 
            "title": "Deploying the Ceph Cluster"
        }, 
        {
            "location": "/Module-1/#configuring-a-ceph-client", 
            "text": "By default Ceph monitor nodes are authorized to run Ceph administrative commands. For the sake of understanding how Ceph client is configured, In this section we will configure a  mgmt  node as our Ceph client node.   On the  mgmt node install  ceph-common  package which provides the  Ceph CLI  and other tools   $ sudo yum install -y ceph-common   Change ownership of the  /etc/ceph  directory   $ sudo chown -R ceph:ceph /etc/ceph   From  mon-node1  copy the Ceph configuration file ( ceph.conf ) and the Ceph administration keyring ( ceph.client.admin.keyring ) to the  mgmt  node   $ ssh mon-node1 -t cat /etc/ceph/ceph.conf | tee /etc/ceph/ceph.conf  $ ssh mon-node1 -t cat /etc/ceph/ceph.client.admin.keyring | tee /etc/ceph/ceph.client.admin.keyring  $ chmod 400 /etc/ceph/ceph.client.admin.keyring\n$ sudo chown -R ceph:ceph /etc/ceph   Verify  mgmt  node which is our Ceph client , can run Ceph commands   [ceph@mgmt ~]$ ceph -s\n    cluster 32ab020c-e510-4884-ab0a-63944c2c6b35\n     health HEALTH_WARN\n            too few PGs per OSD (21   min 30)\n     monmap e1: 3 mons at {mon-node1=10.100.2.11:6789/0,mon-node2=10.100.2.12:6789/0,mon-node3=10.100.2.13:6789/0}\n            election epoch 6, quorum 0,1,2 mon-node1,mon-node2,mon-node3\n     osdmap e20: 9 osds: 9 up, 9 in\n            flags sortbitwise\n      pgmap v33: 64 pgs, 1 pools, 0 bytes data, 0 objects\n            300 MB used, 863 GB / 863 GB avail\n                  64 active+clean\n[ceph@mgmt ~]$", 
            "title": "Configuring a Ceph client"
        }, 
        {
            "location": "/Module-1/#interacting-with-the-ceph-cluster", 
            "text": "In this section we will learn a few commands to interact with our Ceph cluster. These commands should be executed from  mon-node1  node.   ssh to  mon-node1     $ ssh mon-node1   Check cluster status   $ ceph -s  The Above cluster status command shows that the cluster health is not OK and the cluster is complaining about low PG numbers. \nLet's now try to fix this warning.   Verify  pg_num  for default pool  rbd     $ ceph osd dump | grep -i pool   Increase  pg_num   for  rbd  pool to 128 and check cluster status    $ ceph osd pool set rbd pg_num 128\n$ ceph -s   Once the cluster is not creating new PGs , increase  pgp_num  for  rbd  pool to 128 and check cluster status. Your cluster health should now report  HEALTH_OK   $ ceph osd pool set rbd pgp_num 128\n$ ceph -s   Check Ceph OSD stats and tree view of OSDs in cluster   $ ceph osd stat\n$ ceph osd tree   Check Ceph monitor status   $ ceph mon stat   List and check Ceph pool status   $ ceph osd lspools\n$ ceph df\n$ ceph osd dump | grep -i pool   We have reached the end of Module-1. At this point you have learned to deploy, configure and interact with your Ceph cluster.\nFollow the next module to learn how to access your Ceph cluster as a Block Storage.", 
            "title": "Interacting with the Ceph cluster"
        }, 
        {
            "location": "/Module-2/", 
            "text": "Module 2 - Ceph Block Storage interface\n#\n\n\nIn this module we will learn how to provision block storage using Ceph. We will create thin-provisioned, resizable RADOS Block Device (RBD) volume, which will be mapped to \nclient-node1\n and will be taken into use.\n\n\n\n\nNote\n\n\nBefore proceeding with this module make sure you have completed Module-1 and have a running Ceph cluster.\n\n\n\n\n\n\nFrom \nmgmt\n node  configure \nclient-node1\n  as Ceph client by installing \nceph-common\n package and changing ownership of \n/etc/ceph\n directory. \n\n\n\n\n$ ssh client-node1 -t sudo yum install -y ceph-common\n$ ssh client-node1 -t sudo chown -R ceph:ceph /etc/ceph\n\n\n\n\n\n\nFrom \nmgmt\n node create a block device user named \nclient.rbd\n with necessary permissions on Ceph Monitor and OSDs\n\n\n\n\n$ ceph auth get-or-create client.rbd mon 'allow r' osd 'allow rwx pool=rbd' -o /etc/ceph/ceph.client.rbd.keyring\n\n\n\n\n\n\nFrom \nmgmt\n node copy \nceph.conf\n and \nrbd.keyring\n file to \nclient-node1\n \n\n\n\n\n$ scp /etc/ceph/ceph.conf client-node1:/etc/ceph\n$ scp /etc/ceph/ceph.client.rbd.keyring client-node1:/etc/ceph\n\n\n\n\n\n\nFrom \nclient-node1\n verify Ceph cluster is accessible by \nrbd\n user\n\n\n\n\n$ ssh client-node1\n$ ceph -s --id rbd\n\n\n\n\n\n\nFrom \nclient-node1\n create RBD block device with name \nblock-disk1\n of size \n10G\n\n\n\n\n$ rbd create block-disk1 --size 10240 --image-feature layering --id rbd\n\n\n\n\n\n\nVerify block device that we have just created\n\n\n\n\n$ rbd ls --id rbd\n$ rbd info block-disk1 --id rbd\n\n\n\n\n\n\nLoad and verify RBD kernel module\n\n\n\n\n$ sudo modprobe rbd\n$ lsmod | grep -i rbd\n\n\n\n\n\n\nMap \nblock-disk1\n on \nclient-node1\n \n\n\n\n\n$ sudo rbd map block-disk1 --id rbd\n\n\n\n\n\n\nVerify mapped RBD block device\n\n\n\n\n$ rbd showmapped --id rbd\n\n\n\n\n\n\nMake a note of mapped device name from the above command output , in most of the cases it is \n/dev/rbd0\n. Create \nxfs\n filesystem on this Ceph block device.\n\n\n\n\n$ sudo mkfs.xfs /dev/rbd0\n$ sudo mount /dev/rbd0 /mnt\n$ df -h /mnt\n\n\n\n\n\n\nLets run a quick \ndd\n write test on this block device to verify its accessiblity. \n\n\n\n\n$ sudo dd if=/dev/zero of=/mnt/file1 bs=4M oflag=direct count=500 \n\n\n\n\n\n\nMeanwhile the \ndd\n test is going on, you can watch cluster status using which should report IO operations on Ceph cluster.\n\n\n\n\n$ watch ceph -s --id rbd\n\n\n\n\n\n\nThis is it, we have reached to end of Module-2. In this module you have learned how to provision and consume Ceph block device. Follow the next module to learn how to use Ceph as Object Storage", 
            "title": "Module-2 Ceph Block Storage"
        }, 
        {
            "location": "/Module-2/#module-2-ceph-block-storage-interface", 
            "text": "In this module we will learn how to provision block storage using Ceph. We will create thin-provisioned, resizable RADOS Block Device (RBD) volume, which will be mapped to  client-node1  and will be taken into use.   Note  Before proceeding with this module make sure you have completed Module-1 and have a running Ceph cluster.    From  mgmt  node  configure  client-node1   as Ceph client by installing  ceph-common  package and changing ownership of  /etc/ceph  directory.    $ ssh client-node1 -t sudo yum install -y ceph-common\n$ ssh client-node1 -t sudo chown -R ceph:ceph /etc/ceph   From  mgmt  node create a block device user named  client.rbd  with necessary permissions on Ceph Monitor and OSDs   $ ceph auth get-or-create client.rbd mon 'allow r' osd 'allow rwx pool=rbd' -o /etc/ceph/ceph.client.rbd.keyring   From  mgmt  node copy  ceph.conf  and  rbd.keyring  file to  client-node1     $ scp /etc/ceph/ceph.conf client-node1:/etc/ceph\n$ scp /etc/ceph/ceph.client.rbd.keyring client-node1:/etc/ceph   From  client-node1  verify Ceph cluster is accessible by  rbd  user   $ ssh client-node1\n$ ceph -s --id rbd   From  client-node1  create RBD block device with name  block-disk1  of size  10G   $ rbd create block-disk1 --size 10240 --image-feature layering --id rbd   Verify block device that we have just created   $ rbd ls --id rbd\n$ rbd info block-disk1 --id rbd   Load and verify RBD kernel module   $ sudo modprobe rbd\n$ lsmod | grep -i rbd   Map  block-disk1  on  client-node1     $ sudo rbd map block-disk1 --id rbd   Verify mapped RBD block device   $ rbd showmapped --id rbd   Make a note of mapped device name from the above command output , in most of the cases it is  /dev/rbd0 . Create  xfs  filesystem on this Ceph block device.   $ sudo mkfs.xfs /dev/rbd0\n$ sudo mount /dev/rbd0 /mnt\n$ df -h /mnt   Lets run a quick  dd  write test on this block device to verify its accessiblity.    $ sudo dd if=/dev/zero of=/mnt/file1 bs=4M oflag=direct count=500    Meanwhile the  dd  test is going on, you can watch cluster status using which should report IO operations on Ceph cluster.   $ watch ceph -s --id rbd   This is it, we have reached to end of Module-2. In this module you have learned how to provision and consume Ceph block device. Follow the next module to learn how to use Ceph as Object Storage", 
            "title": "Module 2 - Ceph Block Storage interface"
        }, 
        {
            "location": "/Module-3/", 
            "text": "Module 3 - Ceph Object Storage interface\n#\n\n\nThe Ceph object gateway, also know as the RADOS gateway, is an object storage interface built on top of the librados API to provide applications with a RESTful gateway to Ceph storage clusters. \n\n\nTo access Ceph over object storage interfaces i.e. via \nswift\n or \ns3\n we need to configure Ceph Rados Gateway component. In this module we will configure \nrgw-node1\n as Ceph Rados Gateway and then test \ns3\n and \nswift\n from \nclient-node1\n \n\n\n\n\nNote\n\n\nBefore proceeding with this module make sure you have completed Module-1 and have a running Ceph cluster.\n\n\n\n\nInstalling and configuring Ceph RGW\n#\n\n\n\n\nOn the \nmgmt\n node as \nceph\n user, navigate to the \n/usr/share/ceph-ansible/group_vars\n directory\n\n\n\n\n$ cd /usr/share/ceph-ansible/group_vars\n\n\n\n\n\n\nEdit \nall\n file \n\n\n\n\n$ sudo vi all\n\n\n\n\n\n\nUncomment the \nradosgw_dns_name\nsettting,  and set it to \nrgw-node1\n \n\n\n\n\nradosgw_dns_name: rgw-node1\n\n\n\n\n\n\nUncomment the \nradosgw_frontend\n setting, save and exit from the editor\n\n\n\n\nradosgw_frontend: civetweb\n\n\n\n\n\n\nCreate an \nrgws\n file from \nrgws.sample\n file and open it for editing\n\n\n\n\n$ sudo cp rgws.sample rgws\n$ sudo vi rgws\n\n\n\n\n\n\nUncomment the \ncopy_admin_key\n setting and set it to \ntrue\n \n\n\n\n\ncopy_admin_key: true\n\n\n\n\n\n\nAdd Ceph RGW host to Ansible inventory file. Edit the \n/etc/ansible/hosts\n file\n\n\n\n\n$ sudo vi /etc/ansible/hosts\n\n\n\n\n\n\nAdd the following section to \n/etc/ansible/hosts\n file, save and exit from file editor\n\n\n\n\n[rgws]\nrgw-node1\n\n\n\n\n\n\nYour Ansible inventory file should look like this\n\n\n\n\n[mons]\nmon-node1\nmon-node2\nmon-node3\n[osds]\nosd-node1\nosd-node2\nosd-node3\n[rgws]\nrgw-node1\n\n\n\n\n\n\nNavigate to the Ansible configuration directory \n/usr/share/ceph-ansible/\n\n\n\n\n$ cd /usr/share/ceph-ansible/\n\n\n\n\n\n\nRun the Ansible playbook\n\n\n\n\n$ ansible-playbook site.yml -u ceph\n\n\n\n\n\n\nNote\n\n\nAnsible is idempotent in nature , so there is no harm in running it again. Configuration change will not take place after its initial application.\n\n\n\n\n\n\nOnce Ansible run is completed, make sure there is no failed item under \nPLAY RECAP\n \n\n\nVerify \nceph-radosgw\n service is running on \nrgw-node1\n , also make a note of the port number its running on. It must be 8080\n\n\n\n\n$ ssh rgw-node1 systemctl status ceph-radosgw@rgw.rgw-node1.service\n$ ssh rgw-node1 -t sudo netstat -plunt | grep -i rados\n\n\n\n\n\n\nLogin to the \nrgw-node\n to create Radow Gateway user account which will be used by \nS3\n and \nswift\n API's to access the Ceph storage via object storage interface\n\n\n\n\n$ ssh rgw-node1\n\n\n\n\n\n\nCreate RGW user for \nS3\n access \n\n\n\n\n$ radosgw-admin user create --uid='user1' --display-name='First User' --access-key='S3user1' --secret-key='S3user1key'\n\n\n\n\n\n\nCreate RGW subuser for \nswift\n access\n\n\n\n\n$ radosgw-admin subuser create --uid='user1' --subuser='user1:swift' --secret-key='Swiftuser1key' --access=full\n\n\n\n\n\n\nAt this point you have a running and configured Ceph RGW instance. In the next section we will learned about accessing the Ceph cluster via object storage interfaces.\n\n\n\n\nAccess the Ceph object storage using swift API\n#\n\n\n\n\nLogin to \nclient-node\n\n\n\n\n$ ssh client-node1\n\n\n\n\n\n\nInstall \npython-swiftclient\n \n\n\n\n\n$ sudo pip install python-swiftclient\n\n\n\n\n\n\nUsing swift cli , lets try to list swift containers. Although it will not list anything as there are no containers.\n\n\n\n\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' list\n\n\n\n\n\n\nCreate a swift container named \ncontainer-1\n and then list it\n\n\n\n\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' post container-1\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' list\n\n\n\n\n\n\nCreate a dummy file and then upload this file to \ncontainer-1\n using swift\n\n\n\n\n$ base64 /dev/urandom | head -c 10000000 \n dummy_file1.txt\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' upload container-1 dummy_file1.txt\n\n\n\n\n\n\nList \ncontainer-1\n to verify files are getting stored\n\n\n\n\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' list container-1\n\n\n\n\n\n\nEasy right !! So you have just learned how to use Ceph as Object Storage System using swift APIs. Follow the next section to learn how S3 can be used with Ceph.\n\n\n\n\nAccess the Ceph object storage using S3 API\n#\n\n\nCeph object storage cluster can be accessed by any client which talks \nS3\n API.  In this section we will use \ns3cmd\n which has already been installed on \nclient-node1\n machine.\n\n\n\n\nLogin to the \nclient-node1\n\n\n\n\n$ ssh client-node1\n\n\n\n\n\n\nTo use Ceph with S3-style subdomains (e.g., bucket-name.domain-name.com), you need to add a wildcard to the DNS record of the DNS server you use with the ceph-radosgw daemon. We will install \ndnsmasq\n which is a lightweight DNS server.\n\n\n\n\n$ sudo yum install -y dnsmasq\n\n\n\n\n\n\nConfigure dnsmasq to resolve subdomains to Ceph RGW address and start dnsmasq service\n\n\n\n\n$ echo \naddress=/.rgw-node1/10.100.2.15\n | sudo tee -a /etc/dnsmasq.conf\n$ sudo systemctl start dnsmasq\n$ sudo systemctl enable dnsmasq\n\n\n\n\n\n\nEdit \n/etc/resolv.conf\n  and add \nnameserver 127.0.0.1\n \njust after\n \nsearch ec2.internal\n line\n\n\n\n\n# Generated by NetworkManager\nsearch ec2.internal\nnameserver 127.0.0.1\nnameserver 10.100.0.2\n\n\n\n\n\n\nMake sure for any RGW subdomain \nclient-node1\n is resolving \nrgw-node1\n address\n\n\n\n\n$ ping -c 1 anything.rgw-node1\n$ ping -c 1 mybucket.rgw-node1\n$ ping -c 1 mars.rgw-node1\n\n\n\n\n\n\nNext we will configure \ns3cmd\n \n\n\n\n\n$ s3cmd --configure\n\n\n\n\n\n\ns3cmd\n configuration will prompt to enter details, use the following  values for configuration.  Enter the default values for most of the prompts. At this point we \ndo not want\n to use \nHTTPS\n and \ntest configuration\n settings so do not set them \nyes\n. But \nwe do want\n to save settings.\n\n\n\n\nAccess Key: S3user1\nSecret Key: S3user1key\n\nDefault Region [US]: \n Just hit Enter \n\nEncryption password: \n Just hit Enter \n\nPath to GPG program [/usr/bin/gpg]: \n Just hit Enter \n\n\nUse HTTPS protocol [Yes]: No\nHTTP Proxy server name: \n Just hit Enter \n\n\nTest access with supplied credentials? [Y/n] n\nSave settings? [y/N] y\n\n\n\n\n\n\nSample \ns3cmd --configuration\n wizard \n\n\n\n\n[ceph@client-node1 ~]$ s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: S3user1\nSecret Key: S3user1key\nDefault Region [US]:\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program [/usr/bin/gpg]:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: No\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: S3user1\n  Secret Key: S3user1key\n  Default Region: US\n  Encryption password:\n  Path to GPG program: /usr/bin/gpg\n  Use HTTPS protocol: No\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n\nTest access with supplied credentials? [Y/n] n\n\nSave settings? [y/N] y\nConfiguration saved to '/home/ceph/.s3cfg'\n[ceph@client-node1 ~]$\n\n\n\n\n\n\nNext edit \n/home/ceph/.s3cfg\n file, update \nhost_base\n and \nhost_bucket\n as shown below. Save and exit editor\n\n\n\n\nhost_base = rgw-node1:8080\nhost_bucket = %(bucket)s.rgw-node1:8080\n\n\n\n\n\n\nTest Ceph object storage via S3 protocol by listing buckets using \ns3cmd\n. It should list buckets that you have created using \nswift\n in the last section \n\n\n\n\n$ s3cmd ls\n\n\n\n\n\n\nCreate a new bucket\n\n\n\n\n$ s3cmd mb s3://s3-bucket\n$ s3cmd ls\n\n\n\n\n\n\nCreate a dummy file and then upload this file to \ns3-bucket\n via S3 API\n\n\n\n\n$ base64 /dev/urandom | head -c 10000000 \n dummy_file2.txt\n$ s3cmd put dummy_file2.txt s3://s3-bucket\n$ s3cmd ls s3://s3-bucket\n\n\n\n\n\n\nAll done !! Great !!\n\nWe have reached the end of Module-3. In this module you have learned to use the Ceph cluster as object storage using S3 and Swift APIs. You application will use the same procedure to storage objects to Ceph cluster.", 
            "title": "Module-3 Ceph Object Storage"
        }, 
        {
            "location": "/Module-3/#module-3-ceph-object-storage-interface", 
            "text": "The Ceph object gateway, also know as the RADOS gateway, is an object storage interface built on top of the librados API to provide applications with a RESTful gateway to Ceph storage clusters.   To access Ceph over object storage interfaces i.e. via  swift  or  s3  we need to configure Ceph Rados Gateway component. In this module we will configure  rgw-node1  as Ceph Rados Gateway and then test  s3  and  swift  from  client-node1     Note  Before proceeding with this module make sure you have completed Module-1 and have a running Ceph cluster.", 
            "title": "Module 3 - Ceph Object Storage interface"
        }, 
        {
            "location": "/Module-3/#installing-and-configuring-ceph-rgw", 
            "text": "On the  mgmt  node as  ceph  user, navigate to the  /usr/share/ceph-ansible/group_vars  directory   $ cd /usr/share/ceph-ansible/group_vars   Edit  all  file    $ sudo vi all   Uncomment the  radosgw_dns_name settting,  and set it to  rgw-node1     radosgw_dns_name: rgw-node1   Uncomment the  radosgw_frontend  setting, save and exit from the editor   radosgw_frontend: civetweb   Create an  rgws  file from  rgws.sample  file and open it for editing   $ sudo cp rgws.sample rgws\n$ sudo vi rgws   Uncomment the  copy_admin_key  setting and set it to  true     copy_admin_key: true   Add Ceph RGW host to Ansible inventory file. Edit the  /etc/ansible/hosts  file   $ sudo vi /etc/ansible/hosts   Add the following section to  /etc/ansible/hosts  file, save and exit from file editor   [rgws]\nrgw-node1   Your Ansible inventory file should look like this   [mons]\nmon-node1\nmon-node2\nmon-node3\n[osds]\nosd-node1\nosd-node2\nosd-node3\n[rgws]\nrgw-node1   Navigate to the Ansible configuration directory  /usr/share/ceph-ansible/   $ cd /usr/share/ceph-ansible/   Run the Ansible playbook   $ ansible-playbook site.yml -u ceph   Note  Ansible is idempotent in nature , so there is no harm in running it again. Configuration change will not take place after its initial application.    Once Ansible run is completed, make sure there is no failed item under  PLAY RECAP    Verify  ceph-radosgw  service is running on  rgw-node1  , also make a note of the port number its running on. It must be 8080   $ ssh rgw-node1 systemctl status ceph-radosgw@rgw.rgw-node1.service\n$ ssh rgw-node1 -t sudo netstat -plunt | grep -i rados   Login to the  rgw-node  to create Radow Gateway user account which will be used by  S3  and  swift  API's to access the Ceph storage via object storage interface   $ ssh rgw-node1   Create RGW user for  S3  access    $ radosgw-admin user create --uid='user1' --display-name='First User' --access-key='S3user1' --secret-key='S3user1key'   Create RGW subuser for  swift  access   $ radosgw-admin subuser create --uid='user1' --subuser='user1:swift' --secret-key='Swiftuser1key' --access=full   At this point you have a running and configured Ceph RGW instance. In the next section we will learned about accessing the Ceph cluster via object storage interfaces.", 
            "title": "Installing and configuring Ceph RGW"
        }, 
        {
            "location": "/Module-3/#access-the-ceph-object-storage-using-swift-api", 
            "text": "Login to  client-node   $ ssh client-node1   Install  python-swiftclient     $ sudo pip install python-swiftclient   Using swift cli , lets try to list swift containers. Although it will not list anything as there are no containers.   $ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' list   Create a swift container named  container-1  and then list it   $ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' post container-1\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' list   Create a dummy file and then upload this file to  container-1  using swift   $ base64 /dev/urandom | head -c 10000000   dummy_file1.txt\n$ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' upload container-1 dummy_file1.txt   List  container-1  to verify files are getting stored   $ swift -A http://rgw-node1:8080/auth/1.0  -U user1:swift -K 'Swiftuser1key' list container-1   Easy right !! So you have just learned how to use Ceph as Object Storage System using swift APIs. Follow the next section to learn how S3 can be used with Ceph.", 
            "title": "Access the Ceph object storage using swift API"
        }, 
        {
            "location": "/Module-3/#access-the-ceph-object-storage-using-s3-api", 
            "text": "Ceph object storage cluster can be accessed by any client which talks  S3  API.  In this section we will use  s3cmd  which has already been installed on  client-node1  machine.   Login to the  client-node1   $ ssh client-node1   To use Ceph with S3-style subdomains (e.g., bucket-name.domain-name.com), you need to add a wildcard to the DNS record of the DNS server you use with the ceph-radosgw daemon. We will install  dnsmasq  which is a lightweight DNS server.   $ sudo yum install -y dnsmasq   Configure dnsmasq to resolve subdomains to Ceph RGW address and start dnsmasq service   $ echo  address=/.rgw-node1/10.100.2.15  | sudo tee -a /etc/dnsmasq.conf\n$ sudo systemctl start dnsmasq\n$ sudo systemctl enable dnsmasq   Edit  /etc/resolv.conf   and add  nameserver 127.0.0.1   just after   search ec2.internal  line   # Generated by NetworkManager\nsearch ec2.internal\nnameserver 127.0.0.1\nnameserver 10.100.0.2   Make sure for any RGW subdomain  client-node1  is resolving  rgw-node1  address   $ ping -c 1 anything.rgw-node1\n$ ping -c 1 mybucket.rgw-node1\n$ ping -c 1 mars.rgw-node1   Next we will configure  s3cmd     $ s3cmd --configure   s3cmd  configuration will prompt to enter details, use the following  values for configuration.  Enter the default values for most of the prompts. At this point we  do not want  to use  HTTPS  and  test configuration  settings so do not set them  yes . But  we do want  to save settings.   Access Key: S3user1\nSecret Key: S3user1key\n\nDefault Region [US]:   Just hit Enter  \nEncryption password:   Just hit Enter  \nPath to GPG program [/usr/bin/gpg]:   Just hit Enter  \n\nUse HTTPS protocol [Yes]: No\nHTTP Proxy server name:   Just hit Enter  \n\nTest access with supplied credentials? [Y/n] n\nSave settings? [y/N] y   Sample  s3cmd --configuration  wizard    [ceph@client-node1 ~]$ s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: S3user1\nSecret Key: S3user1key\nDefault Region [US]:\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program [/usr/bin/gpg]:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: No\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: S3user1\n  Secret Key: S3user1key\n  Default Region: US\n  Encryption password:\n  Path to GPG program: /usr/bin/gpg\n  Use HTTPS protocol: No\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n\nTest access with supplied credentials? [Y/n] n\n\nSave settings? [y/N] y\nConfiguration saved to '/home/ceph/.s3cfg'\n[ceph@client-node1 ~]$   Next edit  /home/ceph/.s3cfg  file, update  host_base  and  host_bucket  as shown below. Save and exit editor   host_base = rgw-node1:8080\nhost_bucket = %(bucket)s.rgw-node1:8080   Test Ceph object storage via S3 protocol by listing buckets using  s3cmd . It should list buckets that you have created using  swift  in the last section    $ s3cmd ls   Create a new bucket   $ s3cmd mb s3://s3-bucket\n$ s3cmd ls   Create a dummy file and then upload this file to  s3-bucket  via S3 API   $ base64 /dev/urandom | head -c 10000000   dummy_file2.txt\n$ s3cmd put dummy_file2.txt s3://s3-bucket\n$ s3cmd ls s3://s3-bucket   All done !! Great !! \nWe have reached the end of Module-3. In this module you have learned to use the Ceph cluster as object storage using S3 and Swift APIs. You application will use the same procedure to storage objects to Ceph cluster.", 
            "title": "Access the Ceph object storage using S3 API"
        }, 
        {
            "location": "/Module-4/", 
            "text": "Module 4 - Scaling up a Ceph cluster\n#\n\n\n\n\nNote\n\n\n\n\nBefore proceeding with this module make sure you have completed Module-1 and have a running Ceph cluster.\n\n\n\n\n\n\nCeph is designed to scale to exabyte level. Scaling a Ceph cluster is a matter of just adding nodes and Ceph will take care of the rest. In this module we will add a new OSD node \nosd-node4\n with 3 drives to our existing Ceph cluster of 3 OSD nodes. \n\n\n\n\nWe currently have 3 OSD Ceph cluster with total 9 OSDs, lets verify it again\n\n\n\n\n$ ceph osd stat\n$ ceph osd tree\n\n\n\n\nOnce the new node \nosd-node4\n is added we will then have 4 Node Ceph cluster with 12 disks altogether.\n\n\n\n\n\n\nTo scale up Ceph cluster, login to the \nmgmt\n node which is our ceph-ansible node\n\n\n\n\n\n\nEdit \n/etc/ansible/hosts\n file\n\n\n\n\n\n\n$ sudo vi /etc/ansible/hosts\n\n\n\n\n\n\nAdd \nosd-node4\n under \n[osds]\n section. Save and exit from editor\n\n\n\n\n[mons]\nmon-node1\nmon-node2\nmon-node3\n[osds]\nosd-node1\nosd-node2\nosd-node3\nosd-node4\n[rgws]\nrgw-node1\n\n\n\n\n\n\nNavigate to the Ansible configuration directory \n/usr/share/ceph-ansible/\n\n\n\n\n$ cd /usr/share/ceph-ansible/\n\n\n\n\n\n\nNow run Ansible playbook, which will make sure the \nosd-node4\n is added to existing Ceph cluster\n\n\n\n\n$ ansible-playbook site.yml -u ceph\n\n\n\n\n\n\nOnce Ansible run is completed, make sure there is no failed item under \nPLAY RECAP\n \n\n\nSo ceph-ansible has scaled up our Ceph cluster, lets verify it\n\n\n\n\n$ ceph osd stat\n$ ceph osd tree\n$ ceph -s\n\n\n\n\n\n\nAt this point we have scaled up our Ceph cluster to 4 OSD node, without any downtime or service breaks. This shows, how seamless is Ceph scale up operation", 
            "title": "Module 4 - Scaling up a Ceph cluster"
        }, 
        {
            "location": "/Module-4/#module-4-scaling-up-a-ceph-cluster", 
            "text": "Note   Before proceeding with this module make sure you have completed Module-1 and have a running Ceph cluster.    Ceph is designed to scale to exabyte level. Scaling a Ceph cluster is a matter of just adding nodes and Ceph will take care of the rest. In this module we will add a new OSD node  osd-node4  with 3 drives to our existing Ceph cluster of 3 OSD nodes.    We currently have 3 OSD Ceph cluster with total 9 OSDs, lets verify it again   $ ceph osd stat\n$ ceph osd tree  Once the new node  osd-node4  is added we will then have 4 Node Ceph cluster with 12 disks altogether.    To scale up Ceph cluster, login to the  mgmt  node which is our ceph-ansible node    Edit  /etc/ansible/hosts  file    $ sudo vi /etc/ansible/hosts   Add  osd-node4  under  [osds]  section. Save and exit from editor   [mons]\nmon-node1\nmon-node2\nmon-node3\n[osds]\nosd-node1\nosd-node2\nosd-node3\nosd-node4\n[rgws]\nrgw-node1   Navigate to the Ansible configuration directory  /usr/share/ceph-ansible/   $ cd /usr/share/ceph-ansible/   Now run Ansible playbook, which will make sure the  osd-node4  is added to existing Ceph cluster   $ ansible-playbook site.yml -u ceph   Once Ansible run is completed, make sure there is no failed item under  PLAY RECAP    So ceph-ansible has scaled up our Ceph cluster, lets verify it   $ ceph osd stat\n$ ceph osd tree\n$ ceph -s   At this point we have scaled up our Ceph cluster to 4 OSD node, without any downtime or service breaks. This shows, how seamless is Ceph scale up operation", 
            "title": "Module 4 - Scaling up a Ceph cluster"
        }
    ]
}