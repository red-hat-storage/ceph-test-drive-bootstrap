<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="no-js ie6"><![endif]-->
<!--[if IE 7 ]><html class="no-js ie7"><![endif]-->
<!--[if IE 8 ]><html class="no-js ie8"><![endif]-->
<!--[if IE 9 ]><html class="no-js ie9"><![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    
      
        <title>Module-1 Ceph Cluster Setup - Red Hat Ceph Storage Test Driv</title>
      
      
      
        <link rel="canonical" href="https://red-hat-storage.github.io/ceph-test-drive-bootstrap/Module-1/">
      
      
    
    <meta property="og:url" content="https://red-hat-storage.github.io/ceph-test-drive-bootstrap/Module-1/">
    <meta property="og:title" content="Red Hat Ceph Storage Test Driv">
    <meta property="og:image" content="https://red-hat-storage.github.io/ceph-test-drive-bootstrap/Module-1//../images/RedHat.svg">
    <meta name="apple-mobile-web-app-title" content="Red Hat Ceph Storage Test Driv">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
      <link rel="apple-touch-icon" href="../images/RedHat.svg">
    
    
    <link rel="shortcut icon" type="image/x-icon" href="../../images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="../../images/favicon.ico">
    <style>
      @font-face {
      	font-family: 'Icon';
      	src: url('../assets/fonts/icon.eot?52m981');
      	src: url('../assets/fonts/icon.eot?#iefix52m981')
               format('embedded-opentype'),
      		   url('../assets/fonts/icon.woff?52m981')
               format('woff'),
      		   url('../assets/fonts/icon.ttf?52m981')
               format('truetype'),
      		   url('../assets/fonts/icon.svg?52m981#icon')
               format('svg');
      	font-weight: normal;
      	font-style: normal;
      }
    </style>
    <link rel="stylesheet" href="../assets/stylesheets/application-a422ff04cc.css">
    
    
      
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu+Mono">
      <style>
        body, input {
          font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
        }
        pre, code {
          font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
        }
      </style>
    
    
    <script src="../assets/javascripts/modernizr-4ab42b99fd.js"></script>
    
  </head>
  
  
  
  <body class=" ">
    
    <div class="backdrop">
      <div class="backdrop-paper"></div>
    </div>
    <input class="toggle" type="checkbox" id="toggle-drawer">
    <input class="toggle" type="checkbox" id="toggle-search">
    <label class="toggle-button overlay" for="toggle-drawer"></label>
    <header class="header">
      <nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        
          <span class="path">
            
              
                Modules <i class="icon icon-link"></i>
              
            
          </span>
        
        Module-1 Ceph Cluster Setup
      </div>
    </div>
    
    
    <div class="button button-search" role="button" aria-label="Search">
      <label class="toggle-button icon icon-search" title="Search" for="toggle-search"></label>
    </div>
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
    </header>
    <main class="main">
      
      <div class="drawer">
        <nav aria-label="Navigation">
  
  <a href="/" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="../images/RedHat.svg">
        </div>
      
      <div class="name">
        <strong>
          Red Hat Ceph Storage Test Driv
          <span class="version">
            
          </span>
        </strong>
        
      </div>
    </div>
  </a>
  <div class="scrollable">
    <div class="wrapper">
      
      <div class="toc">
        <ul>
          
            
  <li>
    <a class="" title="Home" href="..">
      Home
    </a>
    
  </li>

          
            
  <li>
    <span class="section">Modules</span>
    <ul>
      
        
  <li>
    <a class="current" title="Module-1 Ceph Cluster Setup" href="./">
      Module-1 Ceph Cluster Setup
    </a>
    
      
        
      
      
        <ul>
          
            <li class="anchor">
              <a title="Installing and setting up ceph-ansible" href="#installing-and-setting-up-ceph-ansible">
                Installing and setting up ceph-ansible
              </a>
            </li>
          
            <li class="anchor">
              <a title="Configuring Ceph Global Settings" href="#configuring-ceph-global-settings">
                Configuring Ceph Global Settings
              </a>
            </li>
          
            <li class="anchor">
              <a title="Configuring Ceph OSD Settings" href="#configuring-ceph-osd-settings">
                Configuring Ceph OSD Settings
              </a>
            </li>
          
            <li class="anchor">
              <a title="Deploying Ceph Cluster" href="#deploying-ceph-cluster">
                Deploying Ceph Cluster
              </a>
            </li>
          
            <li class="anchor">
              <a title="Configuring Ceph client" href="#configuring-ceph-client">
                Configuring Ceph client
              </a>
            </li>
          
            <li class="anchor">
              <a title="Interacting with Ceph cluster" href="#interacting-with-ceph-cluster">
                Interacting with Ceph cluster
              </a>
            </li>
          
        </ul>
      
    
  </li>

      
        
  <li>
    <a class="" title="Module-2 Ceph Block Storage" href="../Module-2/">
      Module-2 Ceph Block Storage
    </a>
    
  </li>

      
        
  <li>
    <a class="" title="Module-3 Ceph Object Storage" href="../Module-3/">
      Module-3 Ceph Object Storage
    </a>
    
  </li>

      
    </ul>
  </li>

          
        </ul>
        
      </div>
    </div>
  </div>
</nav>
      </div>
      <article class="article">
        <div class="wrapper">
          
          <h1 id="module-1-setting-up-a-ceph-cluster">Module - 1 : Setting up a Ceph cluster<a class="headerlink" href="#module-1-setting-up-a-ceph-cluster" title="Permanent link">#</a></h1>
<p>RHCS 2.0 has introduced a new and more efficient way to deploy Ceph cluster. Instead of <code>ceph-deploy</code>  RHCS 2.0 ships with <code>ceph-ansible</code> tool which is based on configuration management tool <code>Ansible</code> .</p>
<p>In this module we will deploy a Ceph cluster with 3 OSD nodes and 3 Monitor nodes. We will use <code>ceph-ansible</code> to deploy this cluster.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You must run all the commands using <strong>ceph</strong> user and from <strong>management node</strong>, Unless otherwise specified. </p>
</div>
<ul>
<li>From your workstation login to Ceph management node as <strong><code>ceph</code></strong> user</li>
</ul>
<pre class="code"><code>$ ssh ceph@&lt;IP address of Ceph Management node&gt;</code></pre>


<h2 id="installing-and-setting-up-ceph-ansible">Installing and setting up ceph-ansible<a class="headerlink" href="#installing-and-setting-up-ceph-ansible" title="Permanent link">#</a></h2>
<ul>
<li>Install ceph-ansible package</li>
</ul>
<pre class="code"><code>$ sudo yum install -y ceph-ansible</code></pre>


<ul>
<li>Rename default ansible host file, its of no much use.</li>
</ul>
<pre class="code"><code>$ sudo mv /etc/ansible/hosts /etc/ansible/hosts-default.bkp</code></pre>


<ul>
<li>Create a new ansible hosts file, with the following content.</li>
</ul>
<pre class="code"><code>$ sudo vi /etc/ansible/hosts</code></pre>


<ul>
<li>Under <code>/etc/ansible/hosts</code> file add Ceph monitor host names under <code>[mons]</code>  section and Ceph OSDs host name under <code>[osds]</code>  section . This allows ansible to know which role to be applied on which host.</li>
</ul>
<pre class="code"><code>[mons]
mon-node1
mon-node2
mon-node3
[osds]
osd-node1
osd-node2
osd-node3</code></pre>


<ul>
<li>Create <code>.ansible.cfg</code> file and add <code>host_key_checking = False</code> using the below command</li>
</ul>
<pre class="code"><code>$ echo &quot;[defaults]&quot; &gt;&gt; /home/ceph/.ansible.cfg
$ echo &quot;host_key_checking = False&quot; &gt;&gt; /home/ceph/.ansible.cfg</code></pre>


<ul>
<li>Ensure that Ansible can reach to Ceph hosts.</li>
</ul>
<pre class="code"><code>$ ansible all -m ping</code></pre>


<h2 id="configuring-ceph-global-settings">Configuring Ceph Global Settings<a class="headerlink" href="#configuring-ceph-global-settings" title="Permanent link">#</a></h2>
<ul>
<li>Create a directory under the home directory so Ansible can write the keys</li>
</ul>
<pre class="code"><code>$ cd ~
$ mkdir ceph-ansible-keys</code></pre>


<ul>
<li>Navigate to the Ceph Ansible <code>`group_vars</code> directory</li>
</ul>
<pre class="code"><code>$ cd /usr/share/ceph-ansible/group_vars/</code></pre>


<ul>
<li>Create an <code>all</code> file from the <code>all.sample</code>  file and open it for editing</li>
</ul>
<pre class="code"><code>$ sudo cp all.sample all
$ sudo vi all</code></pre>


<ul>
<li>Uncomment <code>fetch_directory</code> setting under the <code>GENERAL</code> section and point it to directory we created previously for ceph-ansible-keys</li>
</ul>
<pre class="code"><code>fetch_directory: ~/ceph-ansible-keys</code></pre>


<ul>
<li>Under <code>Stable Releases</code> section and <code>ENTERPRISE VERSION RED HAT STORAGE</code> subsection, uncomment <code>ceph_stable_rh_storage</code> setting and set it to <strong><code>true</code></strong></li>
</ul>
<pre class="code"><code>ceph_stable_rh_storage: true</code></pre>


<ul>
<li>Uncomment the <code>ceph_stable_rh_storage_iso_install</code> setting and set it to <strong><code>true</code></strong> </li>
</ul>
<pre class="code"><code>ceph_stable_rh_storage_iso_install: true</code></pre>


<ul>
<li>Uncomment the <code>ceph_stable_rh_storage_iso_path</code> setting and specify the path to RHCS 2.0 ISO image</li>
</ul>
<pre class="code"><code>ceph_stable_rh_storage_iso_path: /home/ceph/rhceph-2.0-rhel-7-x86_64.iso</code></pre>


<ul>
<li>Uncomment the <code>cephx</code> setting under <code>CEPH CONFIGURATION</code> section</li>
</ul>
<pre class="code"><code>cephx: true</code></pre>


<ul>
<li>Uncomment the <code>monitor_interface</code> setting under <code>Monitor options</code> section and specify monitor node interface name.</li>
</ul>
<pre class="code"><code>monitor_interface: eth0</code></pre>


<ul>
<li>Set the <code>journal_size</code>  setting</li>
</ul>
<pre class="code"><code>journal_size: 4096</code></pre>


<ul>
<li>Set the <code>public_network</code> and <code>cluster_network</code> settings</li>
</ul>
<pre class="code"><code>public_network: 10.100.2.0/24
cluster_network: 10.100.1.0/24</code></pre>


<ul>
<li>Save the file and exit from editor</li>
</ul>
<h2 id="configuring-ceph-osd-settings">Configuring Ceph OSD Settings<a class="headerlink" href="#configuring-ceph-osd-settings" title="Permanent link">#</a></h2>
<ul>
<li>To disk devices as Ceph OSD, verify disks logical names. In most cases disk name should be <code>xvdb</code> <code>xvdc</code> and <code>xvdd</code> </li>
</ul>
<pre class="code"><code>$ ssh osd-node1 lsblk</code></pre>


<ul>
<li>To define Ceph OSDs , navigate to the <code>/usr/share/ceph-ansible/group_vars/</code>  directory</li>
</ul>
<pre class="code"><code>$ cd /usr/share/ceph-ansible/group_vars/</code></pre>


<ul>
<li>Create an <code>osds</code> file from <code>osds.sample</code> file and open it for editing</li>
</ul>
<pre class="code"><code>$ sudo cp osds.sample osds
$ sudo vi osds</code></pre>


<ul>
<li>Uncomment the <code>crush_location</code> setting and the <code>osd_crush_location</code> setting</li>
</ul>
<pre class="code"><code>crush_location: false
osd_crush_location: &quot;'root={{ ceph_crush_root }} rack={{ ceph_crush_rack }} host={{ ansible_hostname }}'&quot;</code></pre>


<ul>
<li>To add OSD devices, uncomment the <code>devices:</code> section and add the OSD devices logical name <code>/dev/xvdb</code> and <code>/dev/xvdc</code> and <code>/dev/xvdd</code> to the list of devices</li>
</ul>
<pre class="code"><code>devices:
  - /dev/xvdb
  - /dev/xvdc
  - /dev/xvdd</code></pre>


<ul>
<li>Uncomment the <code>journal_collocation</code> setting and specify <strong><code>true</code></strong> so that OSDs can use co-located journals</li>
</ul>
<pre class="code"><code>journal_collocation: true</code></pre>


<h2 id="deploying-ceph-cluster">Deploying Ceph Cluster<a class="headerlink" href="#deploying-ceph-cluster" title="Permanent link">#</a></h2>
<ul>
<li>Navigate to the <code>ceph-ansible</code> configuration directory</li>
</ul>
<pre class="code"><code>$ cd /usr/share/ceph-ansible</code></pre>


<ul>
<li>Create a <code>site.yml</code> file from the <code>site.yml.sample</code> file </li>
</ul>
<pre class="code"><code>$ sudo cp site.yml.sample site.yml</code></pre>


<ul>
<li>Run the Ansible playbook</li>
</ul>
<pre class="code"><code>$ ansible-playbook site.yml -u ceph</code></pre>


<ul>
<li>Ansible will take a few minutes to complete Ceph deployment. Once its completed Ansible play recap should look similar to this. Make sure Ansible Play Recap does not show any host run failed.</li>
</ul>
<pre class="code"><code>PLAY RECAP ********************************************************************
mon-node1                  : ok=91   changed=19   unreachable=0    failed=0
mon-node2                  : ok=91   changed=18   unreachable=0    failed=0
mon-node3                  : ok=91   changed=18   unreachable=0    failed=0
osd-node1                  : ok=164  changed=16   unreachable=0    failed=0
osd-node2                  : ok=164  changed=16   unreachable=0    failed=0
osd-node3                  : ok=164  changed=16   unreachable=0    failed=0</code></pre>


<ul>
<li>Finally check status of your cluster. </li>
</ul>
<pre class="code"><code>$ ssh mon-node1 ceph -s</code></pre>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this point ignore any cluster health warnings, We will take care of them later in this module.</p>
</div>
<blockquote>
<p><strong>Upto this point you should have a running Ceph cluster with 3 Ceph OSD nodes ( 9 OSDs total ) and 3 Ceph Monitor nodes.</strong> </p>
</blockquote>
<h2 id="configuring-ceph-client">Configuring Ceph client<a class="headerlink" href="#configuring-ceph-client" title="Permanent link">#</a></h2>
<p>By default Ceph monitor nodes are authorized to run Ceph administrative commands. For the sake of understanding how Ceph client is configured, In this section we will configure <code>mgmt</code> node as our Ceph client node.</p>
<ul>
<li>On <code>mgmt</code>node install <code>ceph-common</code> package which provides <code>Ceph CLI</code>  and other tools</li>
</ul>
<pre class="code"><code>$ sudo yum install -y ceph-common</code></pre>


<ul>
<li>Change ownership of <code>/etc/ceph</code> directory</li>
</ul>
<pre class="code"><code>$ sudo chown -R ceph:ceph /etc/ceph</code></pre>


<ul>
<li>From <code>mon-node1</code> copy Ceph configuration file (<code>ceph.conf</code>) and Ceph administration keyring (<code>ceph.client.admin.keyring</code>) to <code>mgmt</code> node</li>
</ul>
<pre class="code"><code>$ ssh mon-node1 -t cat /etc/ceph/ceph.conf | tee /etc/ceph/ceph.conf</code></pre>


<pre class="code"><code>$ ssh mon-node1 -t cat /etc/ceph/ceph.client.admin.keyring | tee /etc/ceph/ceph.client.admin.keyring</code></pre>


<pre class="code"><code>$ chmod 400 /etc/ceph/ceph.client.admin.keyring
$ sudo chown -R ceph:ceph /etc/ceph</code></pre>


<ul>
<li>Verify <code>mgmt</code> node which is our Ceph client , can run Ceph commands</li>
</ul>
<pre class="code"><code>[ceph@mgmt ~]$ ceph -s
    cluster 32ab020c-e510-4884-ab0a-63944c2c6b35
     health HEALTH_WARN
            too few PGs per OSD (21 &lt; min 30)
     monmap e1: 3 mons at {mon-node1=10.100.2.11:6789/0,mon-node2=10.100.2.12:6789/0,mon-node3=10.100.2.13:6789/0}
            election epoch 6, quorum 0,1,2 mon-node1,mon-node2,mon-node3
     osdmap e20: 9 osds: 9 up, 9 in
            flags sortbitwise
      pgmap v33: 64 pgs, 1 pools, 0 bytes data, 0 objects
            300 MB used, 863 GB / 863 GB avail
                  64 active+clean
[ceph@mgmt ~]$</code></pre>


<h2 id="interacting-with-ceph-cluster">Interacting with Ceph cluster<a class="headerlink" href="#interacting-with-ceph-cluster" title="Permanent link">#</a></h2>
<p>In this section we will learn a few commands to interact with Ceph cluster. These commands should be executed from <code>mon-node1</code> node.</p>
<ul>
<li>ssh to <code>mon-node1</code> </li>
</ul>
<pre class="code"><code>$ ssh mon-node1</code></pre>


<ul>
<li>Check cluster status</li>
</ul>
<pre class="code"><code>$ ceph -s</code></pre>


<p>Above cluster status command shows that cluster health is not OK and cluster is complaining about low PG numbers. 
Lets now try to fix this warning.</p>
<ul>
<li>Verify <code>pg_num</code> for default pool <code>rbd</code> </li>
</ul>
<pre class="code"><code>$ ceph osd dump | grep -i pool</code></pre>


<ul>
<li>Increase <code>pg_num</code>  for <code>rbd</code> pool to 128 and check cluster status </li>
</ul>
<pre class="code"><code>$ ceph osd pool set rbd pg_num 128
$ ceph -s</code></pre>


<ul>
<li>Once cluster is not creating new PGs , increase <code>pgp_num</code> for <code>rbd</code> pool to 128 and check cluster status. Your cluster health should now report <code>HEALTH_OK</code></li>
</ul>
<pre class="code"><code>$ ceph osd pool set rbd pgp_num 128
$ ceph -s</code></pre>


<ul>
<li>Check Ceph OSD stats and tree view of OSDs in cluster</li>
</ul>
<pre class="code"><code>$ ceph osd stat
$ ceph osd tree</code></pre>


<ul>
<li>Check Ceph monitor status</li>
</ul>
<pre class="code"><code>$ ceph mon stat</code></pre>


<ul>
<li>List and check Ceph pool status</li>
</ul>
<pre class="code"><code>$ ceph osd lspools
$ ceph df
$ ceph osd dump | grep -i pool</code></pre>


<blockquote>
<p><strong>We have reached to end of Module-1. At this point you have learnt to deploy, configure and interact with Ceph cluster.
Follow the next module to learn accessing Ceph cluster as a Block Storage.</strong></p>
</blockquote>
          <aside class="copyright" role="note">
            
              Copyright Â©2016 Red Hat, Inc. &ndash;
            
            Documentation built with
            <a href="http://www.mkdocs.org" target="_blank">MkDocs</a>
            using the
            <a href="http://squidfunk.github.io/mkdocs-material/" target="_blank">
              Material
            </a>
            theme.
          </aside>
          
            <footer class="footer">
              
  <nav class="pagination" aria-label="Footer">
    <div class="previous">
      
        <a href=".." title="Home">
          <span class="direction">
            Previous
          </span>
          <div class="page">
            <div class="button button-previous" role="button" aria-label="Previous">
              <i class="icon icon-back"></i>
            </div>
            <div class="stretch">
              <div class="title">
                Home
              </div>
            </div>
          </div>
        </a>
      
    </div>
    <div class="next">
      
        <a href="../Module-2/" title="Module-2 Ceph Block Storage">
          <span class="direction">
            Next
          </span>
          <div class="page">
            <div class="stretch">
              <div class="title">
                Module-2 Ceph Block Storage
              </div>
            </div>
            <div class="button button-next" role="button" aria-label="Next">
              <i class="icon icon-forward"></i>
            </div>
          </div>
        </a>
      
    </div>
  </nav>

            </footer>
          
        </div>
      </article>
      <div class="results" role="status" aria-live="polite">
        <div class="scrollable">
          <div class="wrapper">
            <div class="meta"></div>
            <div class="list"></div>
          </div>
        </div>
      </div>
    </main>
    <script>
      var base_url = '..';
      var repo_id  = '';
    </script>
    <script src="../assets/javascripts/application-997097ee0c.js"></script>
    
    
  </body>
</html>